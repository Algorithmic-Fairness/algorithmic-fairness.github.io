<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AlFa Research Lab - Research Projects</title>
  <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
</head>
<body>

<header>
  <div class="logo typed-logo"></div>
  <nav>
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="team.html">Team</a></li>
      <li><a href="publication.html">Publications</a></li>
      <li><a href="projects.html" class="active">Research Projects</a></li>
      <li><a href="data.html">Data & Code</a></li>
	  <li><a href="vacancies.html">Join us</a></li>
    </ul>
  </nav>
</header>

<section id="publications">
  <h1>Research Projects</h1>

  <div class="project-container">
    <!-- Project 1 -->
    <div class="project left">
      <img src="project/project1.png" alt="Project 1 Image">
      <div class="project-info">
        <h2>Fairness in Social Network Analysis: Structural Bias & Algorithms</h2>
        <p>This project investigates how structural biases embedded in social networks
		influence algorithmic outcomes and fairness in AI-driven decision-making. We study
		how network topology, homophily, and community structure amplify inequality, causing 
		underrepresentation of minority or marginalized groups. The project aims to develop fairness-aware
		algorithms for centrality, link prediction, and community detection that address these imbalances. 
		Our broader goal is to establish theoretical and computational foundations for equitable network analysis,
		ensuring that social network algorithms recognize and mitigate bias rather than reinforce it.</p>
      </div>
    </div>

    <!-- Project 2 -->
    <div class="project right">
      <div class="project-info">
        <h2>Fairness Metrics for Community Detection</h2>
        <p>The Fairness Metrics for Community Detection project focuses on developing principled measures to evaluate 
		and ensure fairness in how communities are identified within complex networks. Traditional community detection
		algorithms often produce biased outcomes, overrepresenting or marginalizing certain groups, due to structural 
		or demographic imbalances in the data. To address this, we have proposed novel group and individual 
		fairness metricsthat quantify disparities both across communities and at the node level. Building on this foundation,
		our goal is to extend these metrics to overlapping, multilayer, and hypergraph networks, capturing fairness in 
		richer and more realistic network representations. This project aims to establish a standardized framework for assessing
		and mitigating algorithmic bias in community detection, paving the way toward equitable analysis of human-driven networks.</p>
      </div>
      <img src="project/project2.png" alt="Project 2 Image">
    </div>

    <!-- Project 3 -->
    <div class="project left">
	<img src="project/project3.png" alt="Project 3 Image">
      <div class="project-info">
        <h2> Fairness-aware Fake News Mitigation & Influence Blocking</h2>
        <p>This project investigates how misinformation spreads through social
		networks and how intervention strategies can be made both effective and fair. 
		Traditional influence-blocking algorithms focus solely on maximizing coverage, 
		removing or limiting the spread of fake news, but often ignore the disproportionate suppression
		or amplification of specific demographic groups. This project aims to develop fairness-aware 
		influence-blocking models that minimize misinformation spread while preserving equal representation 
		in communication opportunities.</p>
      </div>
     </div>
	 
	 <!-- Project 4 -->
    <div class="project right">
      <div class="project-info">
        <h2>Algorithmic Fairness in Signed Networks</h2>
        <p>Signed networks—where relationships can be positive (trust, friendship) or negative (conflict, distrust)—
		offer a richer and more realistic view of social and information systems. However, existing AI models often 
		overlook the structural and fairness implications of negative ties. This project investigates how biases emerge
		and propagate in signed networks and develops methods to mitigate them. We propose efficient algorithms to count
		and classify all types of signed triangles, providing a foundation for measuring structural balance and identifying 
		unfair interaction patterns. Building on this, we design fair network embedding techniques that preserve both positive
		and negative relationships while ensuring equitable representation of diverse node groups. The project also involves 
		collecting and curating real-world signed network datasets, enabling the development and benchmarking of fairness-aware
		algorithms for signed networks.</p>
      </div>
      <img src="project/project4.png" alt="Project 4 Image">
    </div>
	
	<!-- Project 5 -->
    <div class="project left">
	<img src="project/project5.png" alt="Project 5 Image">
      <div class="project-info">
        <h2> Deep Reinforcement Learning for Fair Network Analysis</h2>
        <p>The Deep Reinforcement Learning for Fair Network Analysis project investigates how reinforcement learning (RL) can be
		leveraged to promote fairness in complex social network algorithms. The research explores Deep Q-Networks (DQNs) combined 
		with Graph Neural Networks (GNNs) to model network dynamics as a Markov Decision Process (MDP), enabling adaptive decision-making
		that avoids the disproportionate exclusion of minority groups. By learning policies that generalize across problem instances, 
		RL offers a scalable and efficient framework for addressing fairness challenges in network analysis. The project further incorporates
		human feedback (RLHF) to refine reward models and extend the approach to fair rumor blocking and user incentivization. 
		This project further explores policy gradient methods such as Actor–Critic (AC) to design robust, fairness-aware strategies
		for real-world social network scenarios.</p>
      </div>
     </div>
	 
	 <!-- Project 6 -->
    <div class="project right">
      <div class="project-info">
        <h2>Fairness in Graph Neural Networks: Imbalanced Classes & Heterophily</h2>
        <p>The Fairness in Graph Neural Networks: Imbalanced Classes & Heterophily project focuses on uncovering and mitigating bias in graph-based
		deep learning models. Graph Neural Networks (GNNs) have become powerful tools for analyzing relational data, yet their performance and fairness
		often deteriorate when networks exhibit class imbalance or heterophily—situations where connected nodes belong to different classes. 
		These structural challenges can lead to unfair predictions, disproportionately affecting underrepresented groups or minority classes.
		This project develops fairness-aware GNN architectures and training strategies that ensure equitable learning under such complex network 
		conditions. By integrating bias correction, reweighting mechanisms, and fairness constraints into the GNN learning pipeline, we aim to 
		improve both model robustness and ethical accountability.</p>
      </div>
      <img src="project/project6.png" alt="Project 6 Image">
    </div>
	
	<!-- Project 7 -->
    <div class="project left">
	<img src="project/project7.png" alt="Project 7 Image">
      <div class="project-info">
        <h2> Fair Active Learning</h2>
        <p>The Fair Active Learning project focuses on designing machine learning models that can actively select data samples for labeling while
		maintaining fairness across demographic groups. Traditional active learning strategies tend to over-represent majority groups or easily 
		classifiable samples, reinforcing existing biases in data-driven systems. This project aims to develop fairness-aware query selection 
		mechanisms that balance informativeness and representativeness, ensuring equitable model improvement across all populations. 
		By integrating fairness constraints directly into the active learning loop, we seek to create adaptive algorithms that minimize bias
		during model training—resulting in AI systems that are both data-efficient and socially responsible.</p>
      </div>
     </div>
	 
	 <!-- Project 8 -->
    <div class="project right">
      <div class="project-info">
        <h2> Modeling the Impact of Affirmative Actions in Scientific Collaboration </h2>
        <p>The Modeling the Impact of Affirmative Actions in Science project explores how national policies aimed at promoting inclusivity and research
		capacity—commonly known as affirmative actions—shape the global scientific landscape. While developing countries have historically lagged in 
		scientific output due to limited funding and infrastructure, many have recently implemented targeted programs such as international Ph.D. 
		scholarships and return incentives to reverse brain drain and stimulate innovation. Using network science techniques, this project models
		and compares the evolution of co-authorship and collaboration networks across countries with and without affirmative policies, analyzing 
		how such initiatives affect research productivity, international partnerships, and long-term scientific growth. By quantifying these impacts,
		the project aims to uncover causal links between policy interventions and scientific advancement, offering data-driven insights to guide 
		future policy design for equitable and sustainable global research development.</p>
      </div>
      <img src="project/project8.png" alt="Project 8 Image">
    </div>
	
	<!-- Project 9 -->
    <div class="project left">
	<img src="project/project9.png" alt="Project 9 Image">
      <div class="project-info">
        <h2>Algorithmic Profiling and Web Tracking: Privacy, Politics, and Data Governance</h2>
        <p>This project investigates how data collection and algorithmic profiling practices shape user privacy and regulatory landscapes across
		regions. While web tracking enables personalization and targeted advertising, it also fuels large-scale data extraction and deepens 
		surveillance capitalism. This project examines how these dynamics unfold across political and regulatory contexts across different countries.
		By combining technical analyses of tracking mechanisms with policy and governance studies, we explore how varying institutional structures 
		and political orientations influence privacy norms, accountability, and user rights. Ultimately, the project aims to produce empirical insights
		and actionable policy recommendations that advance equitable and transparent governance of data-driven digital ecosystems.</p>
      </div>
     </div>
  </div>
</section>

<footer>
  <p>&copy; 2025 AlFa Lab</p>
</footer>

<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12"></script>
<script>
var typedLogo = new Typed('.typed-logo', {
  strings: ["AlFa Research Lab"],
  typeSpeed: 100,
  backSpeed: 50,
  showCursor: false,
  loop: true
});
</script>

</body>
</html>
